the full code for the platform that supports the option to pack a virtual environment into the zip file. This version includes all the necessary imports, error handling, and cleanup processes.

import os
import sys
import zipfile
import tempfile
import importlib
import subprocess
import shutil
import boto3
from flask import Flask, request, jsonify
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Set up AWS region (optional if your EC2 instance is in the same region as your S3 bucket)
aws_region = os.getenv("AWS_REGION")

# Create S3 client without explicit credentials
if aws_region:
    s3_client = boto3.client('s3', region_name=aws_region)
else:
    s3_client = boto3.client('s3')

app = Flask(__name__)

def download_and_extract_agent(s3_path):
    bucket_name, key = s3_path.replace("s3://", "").split("/", 1)

    with tempfile.NamedTemporaryFile(delete=False, suffix='.zip') as temp_file:
        s3_client.download_fileobj(bucket_name, key, temp_file)
        temp_file_path = temp_file.name

    extract_dir = tempfile.mkdtemp()

    with zipfile.ZipFile(temp_file_path, 'r') as zip_ref:
        zip_ref.extractall(extract_dir)

    os.unlink(temp_file_path)

    return extract_dir

def load_agent(agent_dir):
    venv_dir = os.path.join(agent_dir, 'venv')
    if os.path.exists(venv_dir):
        # Use the packed venv
        activate_this = os.path.join(venv_dir, 'bin', 'activate_this.py')
        exec(open(activate_this).read(), {'__file__': activate_this})
    else:
        # Install dependencies from requirements.txt
        requirements_file = os.path.join(agent_dir, 'requirements.txt')
        if os.path.exists(requirements_file):
            subprocess.check_call([sys.executable, "-m", "pip", "install", "-r", requirements_file])

    sys.path.insert(0, agent_dir)

    try:
        agent_module = importlib.import_module('agent')
        agent_class = getattr(agent_module, 'Agent')
        return agent_class()
    except Exception as e:
        print(f"Error loading agent: {str(e)}")
        return None
    finally:
        sys.path.pop(0)

@app.route('/execute_agent', methods=['POST'])
def execute_agent():
    data = request.json
    s3_path = data.get('s3_path')
    agent_input = data.get('input', {})

    if not s3_path:
        return jsonify({"error": "No S3 path provided"}), 400

    agent_dir = None
    try:
        agent_dir = download_and_extract_agent(s3_path)
        agent = load_agent(agent_dir)

        if agent is None:
            return jsonify({"error": "Failed to load agent"}), 500

        result = agent.execute(agent_input)
        return jsonify(result)

    except Exception as e:
        return jsonify({"error": str(e)}), 500

    finally:
        # Clean up
        if agent_dir and os.path.exists(agent_dir):
            shutil.rmtree(agent_dir)

@app.route('/health', methods=['GET'])
def health_check():
    return jsonify({"status": "healthy"}), 200

if __name__ == '__main__':
    app.run(debug=False, host='0.0.0.0', port=5000)



To use this platform:

Save the above code as app.py.
Create a .env file in the same directory with your AWS region (if needed):
CopyAWS_REGION=your_aws_region

Install the required packages:
pip install flask python-dotenv boto3

Run the application:
python app.py


To create an agent with a packed venv:

Create a new directory for your agent
Set up a virtual environment:
python -m venv venv
source venv/bin/activate  # On Windows, use `venv\Scripts\activate`

Install your dependencies:
pip install your-dependencies

Create your agent.py file with an Agent class that has an execute method:

class Agent:
    def __init__(self):
        # Initialize any resources needed by the agent
        pass

    def execute(self, input_data):
        # Process the input and return the result
        return {"result": f"Processed input: {input_data}"}

Zip the entire directory, including the venv folder:
zip -r agent.zip agent.py venv

Upload this zip file to your S3 bucket.

To use an agent, send a POST request to http://your-ec2-instance:5000/execute_agent with a JSON body like this:
{
    "s3_path": "s3://your-bucket-name/path/to/agent.zip",
    "input": {
        "query": "Your input to the agent"
    }
}

This platform now supports both methods of handling dependencies:

Using a requirements.txt file in the zip
Including a pre-built virtual environment in the zip

The platform will automatically detect which method is being used and act accordingly.
Remember to set up the appropriate IAM role for your EC2 instance to access the S3 bucket.
For production use, consider:

Using a production WSGI server like Gunicorn
Implementing more robust error handling and logging
Adding authentication and authorization
Implementing rate limiting and other security measures
Using AWS Elastic Beanstalk or ECS for easier deployment and scaling